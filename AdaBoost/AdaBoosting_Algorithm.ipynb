{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoosting Algorithm From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAdaBoostClassifier:\n",
    "    def __init__(self, n_estimators):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "        self.n_classes = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        weights = np.full(X.shape[0], 1 / X.shape[0])\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            class_models = []\n",
    "            class_alphas = []\n",
    "            \n",
    "            for class_label in range(self.n_classes):\n",
    "                # Create a binary label vector for the current class\n",
    "                binary_labels = np.where(y == class_label, 1, -1)\n",
    "                \n",
    "                # Train a weak classifier\n",
    "                model = DecisionTreeClassifier(max_depth=1)\n",
    "                model.fit(X, binary_labels, sample_weight=weights)\n",
    "                predictions = model.predict(X)\n",
    "                \n",
    "                # Calculate weighted error\n",
    "                weighted_error = np.sum(weights * (predictions != binary_labels))\n",
    "                \n",
    "                # Calculate alpha\n",
    "                alpha = 0.5 * np.log((1 - weighted_error) / (weighted_error + 1e-10)) + np.log(self.n_classes-1)\n",
    "                class_alphas.append(alpha)\n",
    "                \n",
    "                # Update weights\n",
    "                weights = weights * np.exp(-alpha * binary_labels * predictions)\n",
    "                weights /= np.sum(weights)\n",
    "                \n",
    "                class_models.append(model)\n",
    "            \n",
    "            self.alphas.append(class_alphas)\n",
    "            self.models.append(class_models)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Initialize scores for each class\n",
    "        class_scores = np.zeros((self.n_estimators, self.n_classes, X.shape[0]))\n",
    "        \n",
    "        for index, (class_alphas, class_models) in enumerate(zip(self.alphas, self.models)):\n",
    "            for class_label in range(self.n_classes):\n",
    "                class_scores[index][class_label][:] += class_alphas[class_label] * class_models[class_label].predict(X)\n",
    "        \n",
    "        # Make multiclass predictions based on the highest score\n",
    "        predictions = np.argmax(np.sum(np.sign(class_scores), axis=0), axis=0)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "iris = pd.read_csv(\"iris.csv\")\n",
    "iris = iris.drop('Id', axis=1)\n",
    "\n",
    "X = iris.iloc[:, 0:4]\n",
    "y = iris['Species']\n",
    "labels = {item: index for index, item in enumerate(np.unique(y))}\n",
    "y = y.map(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10, test_size=0.2, shuffle=True)\n",
    "\n",
    "model = MyAdaBoostClassifier(n_estimators=5)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer Wisconsin (Diagnostic) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnnThyroid Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmlb import fetch_data\n",
    "\n",
    "# Returns a pandas DataFrame\n",
    "X1, y1 = fetch_data('ann_thyroid', return_X_y=True, local_cache_dir='./')\n",
    "\n",
    "conditions = [y1 == 1, y1 == 2, y1 == 3]\n",
    "values = [0, 1, 2]\n",
    "\n",
    "y1 = np.select(conditions, values, default=y1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=10, test_size=0.2, shuffle=True)\n",
    "\n",
    "model = MyGradientBoostClassifier(n_estimators=0, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(*y_test)\n",
    "print(*predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using AdaBoosting from library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "iris = pd.read_csv(\"iris.csv\")  \n",
    "iris = iris.drop('Id', axis=1)\n",
    "\n",
    "X = iris.iloc[:, 0:4]\n",
    "y = iris['Species']\n",
    "labels = {item: index for index, item in enumerate(np.unique(y))}\n",
    "y = y.map(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10, test_size=0.2, shuffle=True)\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer Wisconsin (Diagnostic) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnnThyroid Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
