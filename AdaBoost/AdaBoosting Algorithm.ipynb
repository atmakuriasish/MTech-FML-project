{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoosting Algorithm From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAdaBoostClassifier:\n",
    "    def __init__(self, n_estimators):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "        self.n_classes = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        weights = np.full(X.shape[0], 1 / X.shape[0])\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            class_models = []\n",
    "            class_alphas = []\n",
    "            \n",
    "            for class_label in range(n_classes):\n",
    "                # Create a binary label vector for the current class\n",
    "                binary_labels = np.where(y == class_label, 1, -1)\n",
    "                \n",
    "                # Train a weak classifier\n",
    "                model = DecisionTreeClassifier(max_depth=1)\n",
    "                model.fit(X, binary_labels, sample_weight=weights)\n",
    "                predictions = model.predict(X)\n",
    "                \n",
    "                # Calculate weighted error\n",
    "                weighted_error = np.sum(weights * (predictions != binary_labels))\n",
    "                \n",
    "                # Calculate alpha\n",
    "                alpha = 0.5 * np.log((1 - weighted_error) / (weighted_error + 1e-10)) + np.log(n_classes-1)\n",
    "                class_alphas.append(alpha)\n",
    "                \n",
    "                # Update weights\n",
    "                weights = weights * np.exp(-alpha * binary_labels * predictions)\n",
    "                weights /= np.sum(weights)\n",
    "                \n",
    "                class_models.append(model)\n",
    "            \n",
    "            self.alphas.append(class_alphas)\n",
    "            self.models.append(class_models)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Initialize scores for each class\n",
    "        class_scores = np.zeros((self.n_estimators, self.n_classes, X.shape[0]))\n",
    "        \n",
    "        for index, (class_alphas, class_models) in enumerate(zip(self.alphas, self.models)):\n",
    "            for class_label in range(self.n_classes):\n",
    "                class_scores[index][class_label][:] += class_alphas[class_label] * class_models[class_label].predict(X)\n",
    "        \n",
    "        # Make multiclass predictions based on the highest score\n",
    "        predictions = np.argmax(np.sum(np.sign(class_scores), axis=0), axis=0)\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Iris Dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10, shuffle=True)\n",
    "\n",
    "# Create an AdaboostMulticlass classifier with 50 weak classifiers\n",
    "n_classes = len(np.unique(y))\n",
    "adaboost = MyAdaBoostClassifier(n_estimators=10)\n",
    "\n",
    "# Fit the model on the training data\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Iris Dataset (csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "iris = pd.read_csv(\"iris.csv\")\n",
    "iris = iris.drop('Id', axis=1)\n",
    "\n",
    "X = iris.iloc[:, 0:4]\n",
    "y = iris['Species']\n",
    "labels = {item: index for index, item in enumerate(np.unique(y))}\n",
    "y = y.map(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10, test_size=0.2, shuffle=True)\n",
    "\n",
    "model = MyAdaBoostClassifier(n_estimators=5)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using AdaBoost from library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "iris = pd.read_csv(\"iris.csv\")  \n",
    "iris = iris.drop('Id', axis=1)\n",
    "\n",
    "X = iris.iloc[:, 0:4]\n",
    "y = iris['Species']\n",
    "labels = {item: index for index, item in enumerate(np.unique(y))}\n",
    "y = y.map(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10, test_size=0.2, shuffle=True)\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class AdaBoostRegressor:\n",
    "    def __init__(self, n_estimators=50, learning_rate=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.estimators = []\n",
    "        self.estimator_weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        sample_weights = np.ones(n_samples) / n_samples  # Initialize sample weights\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Create a new base regressor (DecisionTreeRegressor)\n",
    "            base_regressor = DecisionTreeRegressor(max_depth=1)\n",
    "            base_regressor.fit(X, y, sample_weight=sample_weights)\n",
    "\n",
    "            # Predict using the current base regressor\n",
    "            y_pred = base_regressor.predict(X)\n",
    "\n",
    "            # Calculate the error and update the sample weights\n",
    "            weighted_error = np.sum(sample_weights * np.abs(y - y_pred)) / (np.sum(sample_weights))\n",
    "            estimator_weight = self.learning_rate * np.log((1 - weighted_error) / weighted_error)\n",
    "\n",
    "            # Update the sample weights\n",
    "            sample_weights *= np.exp(estimator_weight * (y - y_pred))\n",
    "\n",
    "            # Normalize the sample weights\n",
    "            sample_weights /= np.sum(sample_weights)\n",
    "\n",
    "            # Append the current base regressor and its weight to the list of estimators\n",
    "            self.estimators.append(base_regressor)\n",
    "            self.estimator_weights.append(estimator_weight)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Initialize the predicted values\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        # Make predictions using the weighted combination of base regressors\n",
    "        for estimator, weight in zip(self.estimators, self.estimator_weights):\n",
    "            y_pred += weight * estimator.predict(X)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate some synthetic regression data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the AdaBoost Regressor\n",
    "adaboost_regressor = AdaBoostRegressor(n_estimators=100, learning_rate=0.1)\n",
    "adaboost_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = adaboost_regressor.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
